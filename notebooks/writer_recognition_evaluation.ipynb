{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "writer-recognition-evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WRITER RECOGNITION EVALUATION NOTEBOOK\n",
        "\n",
        "We have trained two different classification models in the last notebook that were able to classify an image among 31 different authors. But In biometrics, we need feature extractor modules in order to create and extract the relevant features during the enrollment and save them in our gallery. Therefore, what we are going to do in this notebook is to remove the last layer of our model which is the classification layer in order to access the feature layer.\n",
        "Then once we have the feature extractor model, we will perform the all-against-all methods in order to stress the system evaluation."
      ],
      "metadata": {
        "id": "3exQB20doJ3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPARATION\n",
        "In the following, we are going to set some constants in order to use them during the evaluation. We also have mounting google drive in order to save the training logs into our g-drive."
      ],
      "metadata": {
        "id": "JyS3X1nDoz6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MOUNTING GOOGLE DRIVE\n",
        "The first thing to do is to give google colab permission to access our drive so as to save the training checkpoints."
      ],
      "metadata": {
        "id": "WGpVR8l5o65M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uy9-iqpcH_r",
        "outputId": "a77e8e16-6a3f-47fe-ee74-950ff0744486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#MOUNTING GOOGLE DRIVE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',  force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOWNLOAD The DATASET\n",
        "\n",
        "Once we have finished our preprocessing, we have created a `dataset.zip` file from all the raw images. Now we download this file from our drive in order to start the training phase."
      ],
      "metadata": {
        "id": "5aQ37jq2B0Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Download dataset from google drive\n",
        "\n",
        "!gdown --id 1KhERp3hcRVwhaXSJx7w8xXLrCESmAp1w\n",
        "\n",
        "# unzip the archive file\n",
        "!unzip dataset.zip\n",
        "\n",
        "# we don't need the archive file anymore\n",
        "!rm dataset.zip"
      ],
      "metadata": {
        "id": "Kvobuu08WcoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOAD THE TEST SET\n",
        "\n",
        "Now we have everything to start the training phase,\n",
        "Let's define some constants for our deep architecture. Each of the constants has its own application which is summarized in the table below:\n",
        "\n",
        "CONSTANT_NAME | APPLICATION\n",
        "-------------------|------------------\n",
        "BATCH_SIZE       | # samples that will be passed through to the network at one time\n",
        "IMAGE_SIZE       | width of the image \n",
        "IMAGE_SHAPE      | shape of the image (image_height, image_width)\n",
        "N_IDENTITY        | the output of the model, the classification layer of the model\n",
        "N_TEMPLATES.      | shows the number of the templates per identity\n"
      ],
      "metadata": {
        "id": "L2-AFDrBpH-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 500\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SHAPE = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "N_IDENTITY=31\n",
        "N_TEMPLATES = 12"
      ],
      "metadata": {
        "id": "8pLSugN3fewQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_dataset():\n",
        "  \n",
        "  images = []\n",
        "  test_set_path = os.path.join('/content', 'test_set')\n",
        "\n",
        "  for folder in tqdm(os.listdir(test_set_path)):\n",
        "      folder_path = os.path.join(test_set_path, folder)\n",
        "      for image in os.listdir(folder_path):\n",
        "        if any([image.endswith(extention) for extention in ['.png', '.jpeg']]):\n",
        "\n",
        "            image_path = os.path.join(test_set_path, folder, image)\n",
        "        \n",
        "            images.append(img_to_array(load_img(image_path)))\n",
        "  return np.array(images)"
      ],
      "metadata": {
        "id": "EPiUN6SZhIy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = get_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47r8P2hJjRWU",
        "outputId": "776bfe42-9a41-4a58-de72-568a5893302e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [00:02<00:00, 12.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lvwFjoyjUPR",
        "outputId": "c622200d-4e0c-4516-c9af-03fea8ab486a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(372, 500, 500, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VISUALIZE A SAMPLE OF THE TEST SET\n",
        "\n",
        "Let's check a sample from the test set. As we mentioned before, the dataset contains some samples which have been preprocessed using image processing techniques. The same pipeline also has been done for the test set."
      ],
      "metadata": {
        "id": "wNMaTO7ENQbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.preprocessing.image.array_to_img(\n",
        "    test_set[123]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "jTIyv11wN6Dn",
        "outputId": "7716999b-b029-4ee3-e06c-812cbeba1c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=500x500 at 0x7F24FCFFD650>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL ARCHITECTURE\n",
        "Let's bring back our model architecture that we have previously trained. Then what we need to do is to load the corresponding weights from google colab and remove the classification layer. the model:\n",
        "*   contained 4 convolutional layers\n",
        "*   contained2 dense layers\n",
        "*   and an output (classification) layer"
      ],
      "metadata": {
        "id": "zjDbuGemqIED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "################################.    1st CONVOLUTIONAL LAYER     ################################\n",
        "\n",
        "model.add(layers.Conv2D(filters=32, input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), kernel_size=(7,7),strides=(2,2), padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization()) # Batch Normalisation before passing it to the next layer\n",
        "\n",
        "################################.    2nd CONVOLUTIONAL LAYER     ################################\n",
        "\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(5,5), strides=(2,2), padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "################################.    3rd CONVOLUTIONAL LAYER     ################################\n",
        "\n",
        "model.add(layers.Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "################################.    4th CONVOLUTIONAL LAYER     ################################\n",
        "\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(1,1), strides=(1,1), padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "################################    FLATTEN, FOR DENSE LAYER     ################################\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "################################.    DENSE LAYERS: 1st LAYER     ################################\n",
        "\n",
        "model.add(layers.Dense(256, input_shape=(IMAGE_SIZE * IMAGE_SIZE * 3,)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "################################.    DENSE LAYERS: 2nd LAYER     ################################\n",
        "\n",
        "model.add(layers.Dense(256))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "\n",
        "################################.    OUTPUT LAYER: CLASSES    ################################\n",
        "\n",
        "model.add(layers.Dense(N_IDENTITY))\n",
        "model.add(layers.Activation('softmax'))"
      ],
      "metadata": {
        "id": "Ntk45F2Plos9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "E9KmX9VmmB3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIOMETRIC EVALUATION\n",
        "\n",
        "The first thing is to load the model weights from model. Then we need to create a similarity/distance matrix. \n",
        "for this reason, we considered 31 identities with 12 templates per each. So our matrix should have 372 * 372 dimension( 31 * 12 = 372).\n",
        "In the rows we have probes.\n",
        "Each row (we have 372 rows) is a 31 recognition operation. But in the column, we have 372 templates.\n",
        "\n",
        "Let's load the first model weights which was related to the model with only rotation."
      ],
      "metadata": {
        "id": "UF1QkTS4q-Cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEATURE EXTRACTOR MODEL: <b>SOLO ROTATION</b>\n",
        "\n",
        "Let's access to the last layer of feature from our model. It will be used as our feature extractor modules in order to create a feature vector from the samples."
      ],
      "metadata": {
        "id": "shL83Il5stgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/MyDrive/biometric_project/model_solo_rotation/cp-0016.h5')"
      ],
      "metadata": {
        "id": "Q1MN7Oc4l5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_feature_layer = model.get_layer(index=24).output"
      ],
      "metadata": {
        "id": "8xf4Za2gTuER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = keras.Model(\n",
        "    inputs = model.input,\n",
        "    outputs = last_feature_layer\n",
        ")\n",
        "\n",
        "feature_extractor.compile(optimizer=keras.optimizers.Adam(0.02),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "0JNMnflbQVID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEATURE VECTOR\n",
        "\n",
        "We are going to pass the test set into our feature extractor model in order to have a prediction for each test set sample. The model predicts a feature vector for each image. \n",
        "\n",
        "Then we reshape the result into a list of the feature vectors in which each element in the final list is a feature vector corresponding to each sample in the dataset."
      ],
      "metadata": {
        "id": "PQSdrhVbsqQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetures_test_set = feature_extractor.predict(test_set)"
      ],
      "metadata": {
        "id": "Q5R9XvuZtesj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_test_set = fetures_test_set.reshape(test_set.shape[0], -1)"
      ],
      "metadata": {
        "id": "Q-xgZTSfterW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SIMILARITY MATRIX\n",
        "To be able to perform an evaluation task we need to create a similarity/distance matrix. We considered the similarity matrix. to compare two vector similarity we have taken into consideration the two famous cosine and correlation similarities.\n",
        "\n",
        "Let's define two functions for each method."
      ],
      "metadata": {
        "id": "sxGQHgNSuKXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity_matrix(features):\n",
        "    matrix = []\n",
        "    cosine_function = lambda a,b : np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
        "    for feature in features:\n",
        "        similarities = [\n",
        "            cosine_function(feature, x) for x in features\n",
        "        ]\n",
        "\n",
        "        matrix.append(np.array(similarities))\n",
        "    return np.array(matrix)\n",
        "        \n",
        "\n",
        "\n",
        "def correlation_similarity_matrix(features):\n",
        "    return np.corrcoef(features)"
      ],
      "metadata": {
        "id": "wRrPlbB1xolm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity       = cosine_similarity_matrix(feature_test_set)\n",
        "correlation_similarity  = correlation_similarity_matrix(feature_test_set)"
      ],
      "metadata": {
        "id": "f6W-NffGuRZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarity.shape)\n",
        "print(correlation_similarity.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSziNTSo183C",
        "outputId": "c7c34e2a-0a54-4141-f550-08493e245dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(372, 372)\n",
            "(372, 372)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have a 372 * 372 matrix that gives us the ability to perform an all-against-all evaluation task."
      ],
      "metadata": {
        "id": "4uhD6YLavF_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPOSTER/GENUINE SCORES\n",
        "We are going to use the pyeer library. this library is using two different scores as the input. One corresponds to imposters scores, and the other is the genuine scores.\n",
        "\n",
        "To be able to separate those scores we have defined the following function."
      ],
      "metadata": {
        "id": "xx7Xj0envY0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separated_imposters_genuine_scores(similarity_matrix):\n",
        "    genuine_scores = []\n",
        "    imposter_scores = []\n",
        "    for probe in range(0, similarity_matrix.shape[0]): # for each probe of the matrix ===> probe\n",
        "        for identity in range(N_IDENTITY): # loop over the columns based on the identities\n",
        "\n",
        "            identity_templates_indexes = np.arange(identity * N_TEMPLATES, (identity + 1) * N_TEMPLATES) \n",
        "            identity_templates_indexes_excluded = identity_templates_indexes[np.where(identity_templates_indexes!=probe)]\n",
        "            identity_templates_scores  = similarity_matrix[probe, identity_templates_indexes_excluded]\n",
        "\n",
        "            if(identity == probe//N_TEMPLATES):\n",
        "                genuine_scores.append(identity_templates_scores)\n",
        "            else:\n",
        "                imposter_scores.append(identity_templates_scores)\n",
        "\n",
        "    return np.array(genuine_scores), np.array(imposter_scores)"
      ],
      "metadata": {
        "id": "HZQ3V1KLOxeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above will be used as an input for the pyeer library."
      ],
      "metadata": {
        "id": "ZQAJWAA0wEZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONFUSION MATRIX\n",
        "One interesting thing to do is to create a confusion matrix from the scores which will be able to show how good a biometric system is. To create such matrix we can define very similar method from function above, but this time we split the imposter scores and genuine scores into two subset. and consider a threshold for the methods."
      ],
      "metadata": {
        "id": "A6yNoz87wTuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion(similarity_matrix, threshold):\n",
        "    confusion_matrix = np.array([\n",
        "        [0, 0], # [Genuine Match(GM, GA)     False   Non-Match(FNM, FR)]\n",
        "        [0, 0]  # [False   Match(FM, FA)     Genuine Non-Match(GNM, GR)]\n",
        "    ])\n",
        "\n",
        "    for probe in range(0, similarity_matrix.shape[0]): # for each probe of the matrix ===> probe\n",
        "        for identity in range(N_IDENTITY): # loop over the columns based on the identities\n",
        "\n",
        "            # each tamplate of the identity in the matrix has a column index\n",
        "            '''\n",
        "                                        \n",
        "                identity_0 template_0   0 1 2 3 4 5 6 7 8 9 10 11 __ 12 13 14 15 16 17 18 19 20 21 22 23 __ ... 370 371\n",
        "                identity_0 template_1   0 1 2 3 4 5 6 7 8 9 10 11 __ 12 13 14 15 16 17 18 19 20 21 22 23 __ ... 370 371\n",
        "                identity_0 template_3   0 1 2 3 4 5 6 7 8 9 10 11 __ 12 13 14 15 16 17 18 19 20 21 22 23 __ ... 370 371\n",
        "                        .\n",
        "                        .\n",
        "                identity_0 template_11  0 1 2 3 4 5 6 7 8 9 10 11 __ 12 13 14 15 16 17 18 19 20 21 22 23 __ ... 370 371\n",
        "                identity_1 template_0   0 1 2 3 4 5 6 7 8 9 10 11 __ 12 13 14 15 16 17 18 19 20 21 22 23 __ ... 370 371\n",
        "                        .\n",
        "                        .\n",
        "                identity_30 template_11 0 1 2 3 4 5 6 7 8 9 10 11 __ 12 13 14 15 16 17 18 19 20 21 22 23 __ ... 370 371\n",
        "\n",
        "            '''\n",
        "            identity_templates_indexes = np.arange(identity * N_TEMPLATES, (identity + 1) * N_TEMPLATES) \n",
        "            identity_templates_indexes_excluded = identity_templates_indexes[np.where(identity_templates_indexes!=probe)]\n",
        "            identity_templates_scores  = similarity_matrix[probe, identity_templates_indexes_excluded]\n",
        "\n",
        "            if (identity_templates_scores.max() > threshold): # possible genuine acceptance\n",
        "                if(identity == probe//N_TEMPLATES): # check to see whether the identity id is the same probe identity\n",
        "                    confusion_matrix[0, 0] +=1    # Genuine Match or Genuine Acceptance (GM, GA)\n",
        "                else:\n",
        "                    confusion_matrix[1, 0] +=1.   # False Match or False Acceptance (FM, FA)\n",
        "            else: # possible imposters\n",
        "                if(identity == probe//N_TEMPLATES):# check to see whether the identity id is the same probe identity\n",
        "                    confusion_matrix[0, 1] += 1   # False Non-Match or False Rejection (FNM, FR)\n",
        "                else:\n",
        "                    confusion_matrix[1, 1] += 1   # Genuine Non-Match or Genuine Rejection(GNM, GR)\n",
        "        \n",
        "    return confusion_matrix"
      ],
      "metadata": {
        "id": "G-8w_kOQ2Edn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion(correlation_similarity, 0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtPud8qrV69j",
        "outputId": "c92bc7e1-0ae1-4aa9-b388-dadb0ea0cef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  229,   143],\n",
              "       [  522, 10638]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion(cosine_similarity, 0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcJZMtrwKgMe",
        "outputId": "2d43c711-ac99-4a88-e2d0-f29b1c3f9172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  231,   141],\n",
              "       [  612, 10548]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genuine_scores_corr, impostor_scores_corr = separated_imposters_genuine_scores(correlation_similarity)"
      ],
      "metadata": {
        "id": "g4cx3acELxn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genuine_scores_sim, impostor_scores_sim = separated_imposters_genuine_scores(cosine_similarity)"
      ],
      "metadata": {
        "id": "tto7s0DHPPKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(genuine_scores_corr.flatten().shape, impostor_scores_corr.flatten().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug4mewi3PVgV",
        "outputId": "5855eb15-d1f5-4c1c-a30b-7b943f2d787e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4092,) (133920,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(genuine_scores_sim.flatten().shape, impostor_scores_sim.flatten().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSf9VteNPeoA",
        "outputId": "31bfafe5-bf45-4c46-9e80-aabb0c295694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4092,) (133920,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REPORT AND FOM (FIGURES OF MERIT)\n",
        "\n",
        "To evaluate the system we have used the pyeer library in which we are able to perform all-against-all evaluation and plot different biometric figures."
      ],
      "metadata": {
        "id": "rnT_IfzTezhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyeer"
      ],
      "metadata": {
        "id": "hycK70afPrVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.eer_info import get_eer_stats\n",
        "stats_corr_model_rotation = get_eer_stats(\n",
        "    gen_scores = genuine_scores_corr.flatten(),\n",
        "    imp_scores = impostor_scores_corr.flatten() \n",
        ")\n",
        "stats_sim_model_rotation = get_eer_stats(\n",
        "    gen_scores = genuine_scores_sim.flatten(),\n",
        "    imp_scores = impostor_scores_sim.flatten() \n",
        ")"
      ],
      "metadata": {
        "id": "xwoEUPyVPnDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.plot import plot_eer_stats\n",
        "plot_eer_stats(\n",
        "    [stats_corr_model_rotation],\n",
        "    ['model_solo_rotation_correlation'],\n",
        "    line_width=2,\n",
        "    save_plots=True,\n",
        "    lgf_size=15,\n",
        "    dpi=1000,\n",
        "    bins=correlation_similarity.shape[0],\n",
        "    save_path='/content/gdrive/MyDrive/biometric_project/scores/solo_rotation/correlation',\n",
        "    ext='.jpeg'\n",
        ")"
      ],
      "metadata": {
        "id": "IdQdgYQuQqeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.plot import plot_eer_stats\n",
        "plot_eer_stats(\n",
        "    [stats_sim_model_rotation],\n",
        "    ['model_solo_rotation_similarity'],\n",
        "    line_width=2,\n",
        "    save_plots=True,\n",
        "    lgf_size=15,\n",
        "    dpi=1000,\n",
        "    bins=correlation_similarity.shape[0],\n",
        "    save_path='/content/gdrive/MyDrive/biometric_project/scores/solo_rotation/similarity',\n",
        "    ext='.jpeg'\n",
        ")"
      ],
      "metadata": {
        "id": "6S2ownjcSt7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEATURE EXTRACTOR MODEL : <b>MORE AUGMENTATION</b>\n",
        "\n",
        "Let's now load the second model with more augmentation options."
      ],
      "metadata": {
        "id": "kLBYNc0wYC47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/MyDrive/biometric_project/model_full_aug/cp-0018.h5')"
      ],
      "metadata": {
        "id": "26R7ADnBYC5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_feature_layer = model.get_layer(index=24).output"
      ],
      "metadata": {
        "id": "Zk4kCDicYC5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_full_aug = keras.Model(\n",
        "    inputs = model.input,\n",
        "    outputs = last_feature_layer\n",
        ")\n",
        "\n",
        "feature_extractor_full_aug.compile(optimizer=keras.optimizers.Adam(0.02),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mxcUAkLmYC5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEATURE VECTOR: Second model\n",
        "\n",
        "Like before we start extracting the features using the model extractor"
      ],
      "metadata": {
        "id": "uSAKmpLFYC5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetures_test_set_full_aug = feature_extractor_full_aug.predict(test_set)"
      ],
      "metadata": {
        "id": "XorTsLS7YC5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_test_set_full_aug = fetures_test_set_full_aug.reshape(test_set.shape[0], -1)"
      ],
      "metadata": {
        "id": "Xk1wmMabYC5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SIMILARITY MATRIX\n"
      ],
      "metadata": {
        "id": "xkXOYyw2YC5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity_full_aug       = cosine_similarity_matrix(feature_test_set_full_aug)\n",
        "correlation_similarity_full_aug  = correlation_similarity_matrix(feature_test_set_full_aug)"
      ],
      "metadata": {
        "id": "nqnx84HPYC5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correlation_similarity_full_aug.shape)\n",
        "print(cosine_similarity_full_aug.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e1d500-3b41-40bd-bac2-efbb31912ad6",
        "id": "QooxGDWXYC5I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(372, 372)\n",
            "(372, 372)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have a 372 * 372 matrix that gives us the ability to perform an all-against-all evaluation task."
      ],
      "metadata": {
        "id": "i09umqLkYC5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONFUSION MATRIX\n"
      ],
      "metadata": {
        "id": "AkTsBX5pYC5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion(correlation_similarity_full_aug, 0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3834f2-94ba-41a2-e1c2-f71b40ea3476",
        "id": "apqbLOj2YC5J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  308,    64],\n",
              "       [  873, 10287]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion(cosine_similarity_full_aug, 0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a4e0e2-dd8c-4e59-c34c-61787d9d3820",
        "id": "2DjyQO-_YC5J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  340,    32],\n",
              "       [  877, 10283]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genuine_scores_corr_full_aug, impostor_scores_corr_full_aug = separated_imposters_genuine_scores(correlation_similarity_full_aug)"
      ],
      "metadata": {
        "id": "z0Cb3XJdYC5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genuine_scores_sim_full_aug, impostor_scores_sim_full_aug = separated_imposters_genuine_scores(cosine_similarity_full_aug)"
      ],
      "metadata": {
        "id": "CRil_OETYC5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(genuine_scores_corr_full_aug.flatten().shape, impostor_scores_corr_full_aug.flatten().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72ce212-27a4-407e-d39b-d0d924ee4532",
        "id": "vt0IOlz6YC5J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4092,) (133920,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(genuine_scores_sim_full_aug.flatten().shape, impostor_scores_sim_full_aug.flatten().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aaf7a5f-f90d-483e-86cc-b5743dedcf55",
        "id": "jOB1TPAFYC5J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4092,) (133920,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.eer_info import get_eer_stats\n",
        "stats_corr_model_full_aug = get_eer_stats(\n",
        "    gen_scores = genuine_scores_corr_full_aug.flatten(),\n",
        "    imp_scores = impostor_scores_corr_full_aug.flatten() \n",
        ")\n",
        "stats_sim_model_full_aug = get_eer_stats(\n",
        "    gen_scores = genuine_scores_sim_full_aug.flatten(),\n",
        "    imp_scores = impostor_scores_sim_full_aug.flatten() \n",
        ")"
      ],
      "metadata": {
        "id": "OOrzuai0YC5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.plot import plot_eer_stats\n",
        "plot_eer_stats(\n",
        "    [stats_corr_model_full_aug],\n",
        "    ['model_full_aug_correlation'],\n",
        "    line_width=2,\n",
        "    save_plots=True,\n",
        "    lgf_size=15,\n",
        "    dpi=1000,\n",
        "    bins=correlation_similarity_full_aug.shape[0],\n",
        "    save_path='/content/gdrive/MyDrive/biometric_project/scores/full_aug/correlation',\n",
        "    ext='.jpeg'\n",
        ")"
      ],
      "metadata": {
        "id": "lt51T6-_YC5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.plot import plot_eer_stats\n",
        "plot_eer_stats(\n",
        "    [stats_sim_model_full_aug],\n",
        "    ['model_full_aug_similarity'],\n",
        "    line_width=2,\n",
        "    save_plots=True,\n",
        "    lgf_size=15,\n",
        "    dpi=1000,\n",
        "    bins=correlation_similarity_full_aug.shape[0],\n",
        "    save_path='/content/gdrive/MyDrive/biometric_project/scores/full_aug/similarity',\n",
        "    ext='.jpeg'\n",
        ")"
      ],
      "metadata": {
        "id": "YclAx8IDYC5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n"
      ],
      "metadata": {
        "id": "G_MGvOqJS4Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyeer.report import generate_eer_report\n",
        "generate_eer_report(\n",
        "    [stats_corr_model_rotation, stats_sim_model_rotation, stats_corr_model_full_aug, stats_sim_model_full_aug], \n",
        "    ['solo_rotation_correlation', 'solo_rotation_cosine' , 'full_aug_correlation' , 'full_aug_cosine'], \n",
        "    '/content/gdrive/MyDrive/biometric_project/scores/report.html'\n",
        "    )"
      ],
      "metadata": {
        "id": "1HVe5zKeRGb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}